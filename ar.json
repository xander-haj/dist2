{
  "brand": "Distill.Lab",
  "nav": {
    "curriculum": "المنهج الدراسي",
    "intro": "مقدمة",
    "what_is": "ما هو التقطير؟",
    "concepts": "المفاهيم الأساسية",
    "vs": "التقطير مقابل الضغط",
    "sizes": "أحجام النموذج المستهدف",
    "data": "هندسة البيانات",
    "pipelines": "خطوط أنابيب التقطير",
    "eval": "التقييم",
    "tradeoffs": "المقايضات والحالات",
    "glossary": "المصطلحات",
    "play_all": "تشغيل الكل",
    "play_all_label": "قراءة الصفحة",
    "loading_model": "جاري تحميل النموذج:",
    "generating": "جاري التوليد...",
    "playing": "تشغيل",
    "voice_label": "الصوت",
    "voice_auto": "تلقائي (افتراضي)"
  },
  "home": {
    "title": "هندسة نماذج لغوية فعالة عبر التقطير",
    "lead": "دليل شامل من الصفر للاحتراف لتدريب نماذج لغوية صغيرة وفعالة تحاكي أداء العمالقة.",
    "why_title": "لماذا يهم التقطير",
    "why_text": "يتطلب تشغيل نموذج بحجم 70 مليار محدد محليًا أجهزة GPU بحوالي 15,000 دولار. يسمح لك التقطير بضغط هذا الذكاء في نموذج بحجم 7 أو 13 مليار يعمل على GPU استهلاكي واحد أو حتى كمبيوتر محمول متطور، مع الاحتفاظ بـ 90-95% من قدرات المعلم.",
    "path_title": "اختر مسار التعلم الخاص بك",
    "beginner_title": "مسار المبتدئين",
    "beginner_text": "ابدأ من المبادئ الأولى. افهم 'لماذا' و 'كيف' قبل كتابة الكود.",
    "beginner_btn": "ابدأ هنا",
    "advanced_title": "مهندس متقدم",
    "advanced_text": "أنت تعرف النظرية. انتقل مباشرة إلى تفاصيل التنفيذ والمعايير.",
    "advanced_btn": "انتقل إلى خطوط الأنابيب",
    "tip_title": "نصيحة للمطورين",
    "tip_text": "إذا كنت تبني مساعدًا محليًا، لا تبدأ من الصفر. تقطير نموذج 70B (مثل Llama-3-70B) إلى طالب 8B غالبًا ما يعطي نتائج أفضل من الضبط الدقيق لنموذج 8B على بيانات بشرية فقط."
  },
  "what_is": {
    "title": "ما هو تقطير النموذج؟",
    "lead": "تقطير النموذج هو عملية ضغط المعرفة المعقدة لنموذج 'معلم' ضخم في نموذج 'طالب' مدمج، مع الحفاظ على الدقة وتقليل تكلفة الحوسبة بشكل كبير.",
    "intuition_title": "الحدس الأساسي",
    "intuition_text": "تخيل أستاذًا جامعيًا (المعلم) قضى 40 عامًا في دراسة الفيزياء. يعرف المعادلات، ولكن أيضًا الحدس، والحالات الحدية، والمفاهيم الخاطئة الشائعة. الطالب (نموذج الطالب) لا يحتاج لعيش تلك الـ 40 عامًا. يمكنه التعلم مباشرة من محاضرات الأستاذ (التصنيفات الناعمة) والامتحانات المنسقة (مجموعة البيانات).",
    "dl_terms": "في مصطلحات التعلم العميق:",
    "teacher_def": "<strong>المعلم (Teacher):</strong> نموذج ضخم (مثل GPT-4, Llama-3-70B) بدقة عالية ولكن زمن انتقال وتكلفة هائلين.",
    "student_def": "<strong>الطالب (Student):</strong> بنية أصغر (مثل Llama-3-8B, TinyLlama-1B) نريد تدريبها.",
    "knowledge_def": "<strong>المعرفة:</strong> ليس فقط الإجابة النهائية ('نعم/لا')، بل توزيع الاحتمالات عبر جميع الإجابات الممكنة ('المعرفة المظلمة').",
    "why_distill_title": "لماذا التقطير؟",
    "latency_title": "زمن الانتقال",
    "latency_text": "قد يولد نموذج 70B حوالي 5 رموز/ثانية على الأجهزة الاستهلاكية. يمكن لطالب 7B توليد 50-100 رمز/ثانية، مما يتيح الدردشة في الوقت الفعلي.",
    "cost_title": "التكلفة",
    "cost_text": "يتطلب نشر نموذج 70B حوالي 140 جيجابايت VRAM. الطالب 7B يناسب بطاقة RTX 3060 واحدة، مما يقلل التكاليف 10x-100x.",
    "expect_title": "توقعات واقعية",
    "expect_text": "التقطير ليس سحرًا. لن يطابق طالب 1B عمق الاستدلال لمعلم 70B تمامًا. ومع ذلك، لمهام <em>محددة</em> (مثل التلخيص أو تحويل JSON)، يمكن للطالب مطابقة أو تجاوز موثوقية المعلم."
  },
  "concepts": {
    "title": "المفاهيم الأساسية",
    "lead": "لتقطير نموذج بنجاح، يجب أن تفهم الإشارات التي يتم نقلها. الأمر لا يتعلق فقط بالنص؛ بل بتوزيعات الاحتمالات.",
    "logits_title": "1. اللوجيت (Logits) والتصنيفات الناعمة",
    "logits_text_1": "يستخدم التدريب القياسي 'تسميات صلبة' (الحقيقة الأرضية). إذا كانت الصورة لقطة، فالهدف هو 100% قطة، 0% كلب. هذا يهمل المعلومات. الذئب يشبه الكلب أكثر من القطة.",
    "logits_text_2": "<strong>اللوجيت (Logits)</strong> هي الدرجات الخام قبل التسوية. <strong>التصنيفات الناعمة</strong> هي الاحتمالات بعد تطبيق Softmax. قد يقول المعلم: 'هذا 90% كلب، 9% ذئب، 1% قطة'. هذه الـ 9% تخبر الطالب عن التشابه البصري.",
    "temp_title": "الرياضيات: معامل درجة الحرارة ($T$)",
    "temp_desc": "لكشف المزيد من 'المعرفة المظلمة' (الاحتمالات الصغيرة)، نقسم اللوجيت ($z_i$) على درجة حرارة $T$ قبل Softmax:",
    "temp_li_1": "<strong>$T = 1$:</strong> Softmax قياسي. قمم قوية، يخفي الفئات الصغيرة.",
    "temp_li_2": "<strong>$T > 1$:</strong> ينعم التوزيع. يرى الطالب أن 'الذئب' بديل معقول لـ 'الكلب'. هذا حاسم للتقطير.",
    "loss_title": "2. دوال الخسارة",
    "loss_intro": "يجمع التقطير عادة بين دالتين للخسارة:",
    "loss_kl": "<strong>خسارة التقطير (KL Divergence):</strong> تقيس مدى اختلاف توزيع الاحتمالات الناعم للطالب عن توزيع المعلم.",
    "loss_ce": "<strong>خسارة الطالب (Cross-Entropy):</strong> تقيس الدقة مقابل التسميات الصلبة الحقيقية (تضمن عدم انحراف الطالب).",
    "pitfall_title": "فخ: عدم تطابق المفردات",
    "pitfall_text": "يتطلب التقطير القائم على اللوجيت أن يشترك المعلم والطالب في نفس الـ Tokenizer. إذا اختلفا، يجب استخدام <strong>تقطير التعليمات</strong> (التدريب على النص المولد) بدلاً من ذلك."
  },
  "vs": {
    "title": "التقطير مقابل تقنيات الضغط الأخرى",
    "lead": "التقطير ليس الطريقة الوحيدة لتصغير النماذج. غالبًا ما يعمل بشكل أفضل عند دمجه مع التكميم (Quantization) والتقليم (Pruning).",
    "table_title": "جدول المقارنة",
    "th_tech": "التقنية",
    "th_mech": "الآلية",
    "th_pros": "الإيجابيات",
    "th_cons": "السلبيات",
    "distill_desc": "تدريب بنية أصغر لمحاكاة بنية أكبر.",
    "distill_pros": "يغير هيكل البنية؛ استدلال أسرع؛ يحافظ على المنطق جيدًا.",
    "distill_cons": "مكلف في التدريب؛ يتطلب بيانات معلم جيدة.",
    "quant_desc": "تقليل الدقة (مثلاً FP16 إلى INT4).",
    "quant_pros": "تقليل فوري للذاكرة؛ لا يتطلب تدريبًا (عادة).",
    "quant_cons": "فقدان الدقة؛ لا يزيد السرعة بقدر البنية الأصغر.",
    "prune_desc": "إزالة الخلايا العصبية أو الطبقات.",
    "prune_pros": "يمكن أن يخلق نماذج متفرقة.",
    "prune_cons": "غالبًا ما ينتج مصفوفات متفرقة يصعب تسريعها على وحدات GPU القياسية.",
    "lora_desc": "تجميد الأوزان وتدريب محولات صغيرة.",
    "lora_pros": "ذاكرة تدريب رخيصة جدًا.",
    "lora_cons": "لا يقلل حجم الاستدلال (ما لم يتم دمجه، وحتى ذلك الحين حجم النموذج الأساسي ثابت).",
    "pipeline_title": "خط أنابيب 'التقطير ثم التكميم'",
    "pipeline_intro": "المعيار الصناعي للنشر المحلي هو الجمع بين هؤلاء:",
    "step_1": "<strong>التقطير:</strong> تدريب طالب 7B من معلم 70B بدقة FP16.",
    "step_2": "<strong>التكميم:</strong> تحويل الطالب 7B إلى 4-bit (GGUF/EXL2).",
    "pipeline_out": "ينتج عن ذلك نموذج أصغر ماديًا (البنية) وكثيف الذاكرة (الدقة)، وغالبًا ما يعمل أسرع بـ 4-10 مرات من المعلم الأصلي."
  },
  "sizes": {
    "title": "اختيار أحجام النموذج المستهدف",
    "lead": "الخطوة الأولى في التقطير هي اختيار حجم الطالب. يملي هذا بالكامل تقريبًا من قبل أجهزة النشر المستهدفة.",
    "tool_title": "موصي الأجهزة التفاعلي",
    "label_vram": "VRAM المتاحة (GB)",
    "label_use": "حالة الاستخدام الأساسية",
    "opt_chat": "دردشة عامة",
    "opt_code": "مساعد برمجة",
    "opt_rag": "RAG (QA المستندات)",
    "btn_find": "ابحث عن أفضل حجم نموذج",
    "rec_title": "توصية",
    "tiers_title": "تفصيل طبقات الحجم",
    "t1_title": "1B - 3B معامل",
    "t1_target": "<strong>الهدف:</strong> الهواتف، Raspberry Pi، المتصفح (WebLLM).",
    "t1_cap": "<strong>القدرات:</strong> جيد لتصحيح القواعد، التصنيف البسيط، وتنسيق JSON. يعاني مع الاستدلال المعقد. مثالي لمهام ضيقة محددة.",
    "t2_title": "7B - 9B معامل",
    "t2_target": "<strong>الهدف:</strong> معظم أجهزة الكمبيوتر المحمولة الاستهلاكية (M1/M2, GTX 1080+).",
    "t2_cap": "<strong>القدرات:</strong> 'النقطة الحلوة'. قادر على الاستدلال الجيد، والبرمجة، والدردشة. مع تكميم 4-bit، يناسب 6-8GB RAM.",
    "t3_title": "13B - 30B معامل",
    "t3_target": "<strong>الهدف:</strong> أجهزة سطح المكتب مع وحدات GPU مخصصة (يوصى بـ 24GB).",
    "t3_cap": "<strong>القدرات:</strong> أداء قريب من GPT-3.5. ضروري لاتباع التعليمات المعقدة، والكتابة الإبداعية."
  },
  "data": {
    "title": "بيانات التقطير",
    "lead": "البيانات السيئة تؤدي لنتائج سيئة. تعتمد جودة نموذج الطالب بالكامل على جودة البيانات المولدة بواسطة معلمك.",
    "syn_title": "توليد البيانات الاصطناعية",
    "syn_text": "يعتمد معظم التقطير الحديث على 'البيانات الاصطناعية' - استخدام نموذج رائد (GPT-4) لتوليد أمثلة التدريب. هذا أرخص وأسرع من التعليق البشري.",
    "pipe_title": "خط الأنابيب: نهج 'الكتاب المدرسي'",
    "step_seed": "<strong>محفزات البذور:</strong> اكتب 100 مثال عالي الجودة للمهمة (مثل استعلامات SQL).",
    "step_exp": "<strong>التوسع:</strong> اطلب من المعلم 'توليد 10 أمثلة أخرى مثل هذه، ولكن مع تنويع التعقيد والمجال.'",
    "step_filt": "<strong>التصفية:</strong> استخدم المعلم للتحقق من المخرجات. هل ينفذ استعلام SQL بالفعل؟",
    "step_fmt": "<strong>التنسيق:</strong> تحويل إلى تنسيق تدريب الطالب (ChatML, Alpaca).",
    "cot_title": "تقطير سلسلة الأفكار (CoT)",
    "cot_text": "إذا دربت الطالب فقط على الإجابة، فسيحفظ النتيجة. إذا دربته على <em>الاستدلال</em>، فسيتعلم المنطق.",
    "cot_bad": "<strong>بيانات سيئة:</strong> <code>س: 2+2؟ ج: 4</code>",
    "cot_good": "<strong>بيانات جيدة (CoT):</strong> <code>س: 2+2؟ ج: لدينا وحدتان. إضافة وحدتين أخريين ينتج أربع وحدات. الإجابة: 4</code>",
    "cot_res": "تظهر الأبحاث أن تضمين خطوات الاستدلال يحسن بشكل كبير قدرة الطالب على التعميم.",
    "tip_title": "نصيحة: التنوع هو الملك",
    "tip_text": "لا تولد 100,000 مثال بنفس هيكل الجملة. سيعاني الطالب من فرط التخصيص. أعط الأولوية للتنوع في الطول والنغمة والتعقيد."
  },
  "pipelines": {
    "title": "خطوط أنابيب التقطير",
    "lead": "سير عمل موحد لأخذ معلم وإنتاج طالب. انقر فوق الخطوات أدناه لتوسيع التفاصيل.",
    "step_1_title": "1. تحديد النطاق والمعلم",
    "step_1_text": "اختر مهمتك (مثلاً 'مساعد برمجة Python'). اختر معلمًا معروفًا بهذه القدرة (مثلاً GPT-4). قرر حجم الطالب (مثلاً 7B).",
    "step_2_title": "2. تنسيق البيانات",
    "step_2_text": "ولد 10k-50k عينة باستخدام المعلم. قم بتضمين استدلال سلسلة الأفكار. نظف البيانات. قسّم إلى مجموعات تدريب/تحقق.",
    "step_3_title": "3. التدريب (SFT)",
    "step_3_text": "الضبط الدقيق للطالب على مجموعة البيانات. استخدم LoRA إذا كانت الـ GPU ضعيفة. راقب خسارة التحقق لمنع فرط التخصيص.",
    "step_4_title": "4. التقييم والتكرار",
    "step_4_text": "شغل المعايير (HumanEval للكود). إذا كانت الدقة منخفضة، عد للخطوة 2 وحسن جودة البيانات.\""
  },
  "eval": {
    "title": "تقييم النماذج المقطرة",
    "lead": "كيف تعرف ما إذا كان الطالب يتعلم بالفعل، أم يحفظ فقط؟ التقييم متعدد الأبعاد.",
    "quant_title": "1. المقاييس الكمية",
    "metric_ppl": "<strong>الحيرة (Perplexity):</strong> تقيس مدى تفاجئ النموذج بالنص. الأقل أفضل.",
    "metric_bench": "<strong>المعايير:</strong> اختبارات قياسية مثل MMLU (المعرفة العامة) و HumanEval (البرمجة).",
    "judge_title": "2. LLM-كقاضي",
    "judge_text": "استخدم نموذجًا أقوى (مثل GPT-4) لتصنيف إجابات الطالب. هذا يتوسع بشكل أفضل من المراجعة البشرية.",
    "calc_title": "حاسبة المقاييس",
    "est_title": "مقدار تأثير الأداء",
    "est_desc": "قدّر انخفاض الأداء عند الانتقال من المعلم إلى الطالب.",
    "label_teacher": "درجة المعلم (MMLU)",
    "label_ratio": "نسبة الضغط",
    "opt_10": "10% حجم (10:1)",
    "opt_20": "20% حجم (5:1)",
    "opt_50": "50% حجم (2:1)",
    "btn_calc": "حساب درجة الطالب المتوقعة"
  },
  "cases": {
    "title": "المقايضات العملية ودراسات الحالة",
    "lead": "التقطير هو فن التسوية. استكشف كيف تشكل القيود الواقعية القرارات الهندسية.",
    "sim_title": "محاكي الاستراتيجية",
    "sim_desc": "قم بتكوين سيناريو نشر لتلقي توصية هندسية.",
    "label_device": "الجهاز",
    "label_task": "المهمة",
    "label_latency": "زمن الانتقال",
    "opt_mobile": "هاتف (2-4GB RAM)",
    "opt_laptop": "لابتوب (8-16GB RAM)",
    "opt_desktop": "سطح مكتب (24GB VRAM)",
    "opt_simple": "تنسيق/تصنيف بسيط",
    "opt_coding": "برمجة/استدلال",
    "opt_creative": "إبداعي/تحليل",
    "btn_run": "تحليل وتوصية"
  },
  "glossary": {
    "title": "مسرد المصطلحات",
    "search_placeholder": "ابحث عن مصطلحات (مثلاً 'Logits')...",
    "no_results": "لم يتم العثور على مصطلحات.",
    "term_activation": "مخرجات العصبون/الطبقة. يطابق التقطير القائم على الميزات هذه.",
    "term_distillation": "نقل المعرفة من المعلم إلى الطالب عبر تسميات ناعمة.",
    "term_dark_knowledge": "المعلومات المخفية في اللوجيت ذات الاحتمالية المنخفضة.",
    "term_kl": "دالة خسارة تقيس الفرق بين توزيعات احتمالات المعلم/الطالب.",
    "term_logits": "درجات التنبؤ الخام قبل Softmax.",
    "term_temp": "معامل لتنعيم توزيعات الاحتمالات."
  },
  "recommender": {
    "r1_title": "توصية: 1B - 3B (مكمم)",
    "r1_text": "أجهزتك مقيدة للغاية. التزم بنماذج مثل TinyLlama-1.1B. قم بالتكميم إلى 4-bit.",
    "r2_code_title": "توصية: DeepSeek-Coder-6.7B (Q4)",
    "r2_code_text": "للبرمجة، تحتاج على الأقل 6 مليار معامل. مع 8GB VRAM، يمكنك تشغيل نموذج 7B مكمم.",
    "r2_std_title": "توصية: Llama-3-8B (Q4/Q5)",
    "r2_std_text": "نموذج 8B القياسي يناسب تمامًا 8GB VRAM مع تكميم 4-bit.",
    "r3_title": "توصية: 13B - 14B (Q4)",
    "r3_text": "مع 12GB+ VRAM، يمكنك الترقية إلى Mistral-Nemo-12B. تقدم هذه استدلالًا أفضل بكثير.",
    "r4_title": "توصية: 34B - 70B (Q4)",
    "r4_text": "مع 24GB+ VRAM، يمكنك تشغيل Yi-34B أو حتى Llama-3-70B المكمم."
  },
  "simulator": {
    "m_code_title": "⚠️ غير موصى به",
    "m_code_desc": "تفتقر الأجهزة المحمولة إلى ذاكرة الوصول العشوائي لنماذج البرمجة الكفؤة.",
    "m_code_s1": "تفريغ إلى الخادم",
    "m_code_s2": "استخدم نموذج 1B صغير للنحو فقط",
    "m_gen_title": "الهدف: 1B-3B مكمم",
    "m_gen_desc": "استخدم نموذجًا مضغوطًا للغاية مثل TinyLlama.",
    "m_gen_s1": "استخدم تنسيق GGUF (q4_k_m)",
    "m_gen_s2": "توقع 20 t/s",
    "l_title": "الهدف: نموذج 7B-8B",
    "l_desc": "النقطة الحلوة. Llama-3-8B أو Mistral-7B.",
    "l_s1": "استخدم تكميم 4-bit",
    "l_s2": "جيد للدردشة/الاستدلال العام",
    "d_title": "الهدف: نموذج 13B-34B",
    "d_desc": "لديك أجهزة راقية. استخدمها.",
    "d_s1": "Yi-34B أو Command R",
    "d_s2": "FP16 كامل أو تكميم عالي",
    "d_s3": "قادر على الاستدلال العميق"
  },
  "eval_logic": {
    "result_label": "درجة الطالب المتوقعة (مع التقطير):",
    "base_label": "بدون تقطير (تدريب من الصفر):"
  }
}