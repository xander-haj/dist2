{
  "brand": "Distill.Lab (蒸馏实验室)",
  "nav": {
    "curriculum": "课程大纲",
    "intro": "简介",
    "what_is": "什么是蒸馏？",
    "concepts": "核心概念",
    "vs": "蒸馏与压缩",
    "sizes": "目标模型尺寸",
    "data": "数据工程",
    "pipelines": "蒸馏流程",
    "eval": "模型评估",
    "tradeoffs": "权衡与案例",
    "glossary": "术语表",
    "play_all": "播放全部",
    "play_all_label": "朗读本页",
    "loading_model": "加载模型中:",
    "generating": "生成中...",
    "playing": "播放中",
    "voice_label": "语音",
    "voice_auto": "自动 (默认)"
  },
  "home": {
    "title": "通过模型蒸馏打造高效LLM",
    "lead": "一份全面的从零到精通指南，教你训练小巧高效的语言模型，使其模拟前沿大模型的性能。",
    "why_title": "为什么蒸馏很重要",
    "why_text": "在本地运行700亿参数的模型需要约15,000美元的GPU硬件（2x A6000 或 4x 3090）。蒸馏允许你将这种智能压缩到70亿或130亿参数的模型中，使其可以在单张消费级GPU甚至高端笔记本电脑上运行，同时保留教师模型90-95%的特定能力。",
    "path_title": "选择你的学习路径",
    "beginner_title": "初学者路径",
    "beginner_text": "从基本原理开始。在接触代码之前了解“为什么”和“怎么做”。",
    "beginner_btn": "从这里开始",
    "advanced_title": "高级工程师",
    "advanced_text": "你已了解理论。直接跳转到实施细节和基准测试。",
    "advanced_btn": "跳转到流程",
    "tip_title": "开发者提示",
    "tip_text": "如果你正在构建本地助手，不要从零开始。将700亿参数的模型（如Llama-3-70B）蒸馏到80亿参数的学生模型中，通常比仅在人类数据集上微调80亿模型效果更好。"
  },
  "what_is": {
    "title": "什么是模型蒸馏？",
    "lead": "模型蒸馏是将海量“教师”模型的复杂知识压缩到紧凑的“学生”模型中的过程，在保持准确性的同时大幅降低计算成本。",
    "intuition_title": "核心直觉",
    "intuition_text": "想象一位研究物理学40年的大学教授（教师）。他们知道公式，但也知道直觉、边缘情况和常见的误解。一个学生（学生模型）不需要重新经历那40年。他们可以直接从教授的讲座（软标签）和精选的考试（数据集）中学习。",
    "dl_terms": "在深度学习术语中：",
    "teacher_def": "<strong>教师模型 (Teacher)：</strong> 一个海量模型（例如 GPT-4, Llama-3-70B），准确率高但延迟大且成本高。",
    "student_def": "<strong>学生模型 (Student)：</strong> 我们想要训练的较小架构（例如 Llama-3-8B, TinyLlama-1B）。",
    "knowledge_def": "<strong>知识 (Knowledge)：</strong> 不仅仅是最终答案（“是/否”），而是所有可能答案的概率分布（“暗知识”）。",
    "why_distill_title": "为什么要蒸馏？",
    "latency_title": "延迟",
    "latency_text": "70B模型在消费级硬件上可能每秒生成5个token。蒸馏后的7B模型可以每秒生成50-100个token，实现实时聊天和工具使用。",
    "cost_title": "成本",
    "cost_text": "部署70B模型需要约140GB显存（多张A100）。7B学生模型适合单张RTX 3060甚至MacBook Air，部署成本降低10-100倍。",
    "expect_title": "现实期望",
    "expect_text": "蒸馏不是魔法。10亿参数的学生模型永远无法完全匹配700亿教师模型的推理深度。然而，对于<em>特定任务</em>（如总结新闻或转换JSON），学生模型通过专注训练可以匹配甚至超过教师的可靠性。"
  },
  "concepts": {
    "title": "核心概念",
    "lead": "要成功蒸馏模型，你必须理解正在传递的信号。这不仅仅是关于文本；它是关于概率分布。",
    "logits_title": "1. Logits 和软标签 (Soft Labels)",
    "logits_text_1": "标准训练使用“硬标签”（Ground Truth）。如果图像是猫，目标是100%猫，0%狗。然而，这丢失了信息。狼比猫更像狗。",
    "logits_text_2": "<strong>Logits</strong> 是模型在归一化之前输出的原始分数。<strong>软标签</strong> 是应用Softmax后的概率。教师可能会说：“这是90%的狗，9%的狼，1%的猫”。这9%告诉学生关于视觉相似性的信息。",
    "temp_title": "数学：温度参数 ($T$)",
    "temp_desc": "为了暴露更多的“暗知识”（小概率），我们在Softmax之前将logits ($z_i$) 除以温度 $T$：",
    "temp_li_1": "<strong>$T = 1$:</strong> 标准Softmax。峰值强，隐藏小类。",
    "temp_li_2": "<strong>$T > 1$:</strong> 软化分布。学生看到“狼”是“狗”的一个合理替代。这对蒸馏至关重要。",
    "loss_title": "2. 损失函数",
    "loss_intro": "蒸馏通常结合两个损失函数：",
    "loss_kl": "<strong>蒸馏损失 (KL散度)：</strong> 衡量学生的软概率分布与教师的软分布之间的差异。",
    "loss_ce": "<strong>学生损失 (交叉熵)：</strong> 衡量针对硬性真实标签的准确性（确保学生不会仅仅陷入抽象模仿）。",
    "pitfall_title": "陷阱：词汇表不匹配",
    "pitfall_text": "基于Logit的蒸馏要求教师和学生共享完全相同的分词器（Tokenizer）。如果它们不同（例如 Llama-3 vs Mistral），你不能直接匹配logits。你必须改用<strong>指令蒸馏</strong>（在生成的文本上训练）。"
  },
  "vs": {
    "title": "蒸馏与其他压缩技术",
    "lead": "蒸馏不是使模型变小的唯一方法。它通常在与量化和剪枝结合使用时效果最好。",
    "table_title": "对比表",
    "th_tech": "技术",
    "th_mech": "机制",
    "th_pros": "优点",
    "th_cons": "缺点",
    "distill_desc": "训练较小的架构来模仿较大的架构。",
    "distill_pros": "改变架构结构；推理更快；很好地保留逻辑。",
    "distill_cons": "训练昂贵；需要好的教师数据。",
    "quant_desc": "降低精度（例如 FP16 到 INT4）。",
    "quant_pros": "即时减少内存；通常不需要训练。",
    "quant_cons": "精度损失；速度提升不如更小的架构明显。",
    "prune_desc": "移除神经元或层。",
    "prune_pros": "可以创建稀疏模型。",
    "prune_cons": "通常导致稀疏矩阵，难以在标准GPU上加速。",
    "lora_desc": "冻结权重并训练小适配器。",
    "lora_pros": "训练显存非常低。",
    "lora_cons": "不减少推理大小（除非合并，即便如此基础模型大小是固定的）。",
    "pipeline_title": "“先蒸馏后量化”流程",
    "pipeline_intro": "本地部署的行业标准是结合这些技术：",
    "step_1": "<strong>蒸馏：</strong> 使用FP16从70B教师训练7B学生。",
    "step_2": "<strong>量化：</strong> 将7B学生转换为4-bit (GGUF/EXL2)。",
    "pipeline_out": "这导致模型在物理上更小（架构）且内存密集（精度），运行速度通常比原始教师快4-10倍，性能惊人地接近。"
  },
  "sizes": {
    "title": "选择目标模型尺寸",
    "lead": "蒸馏的第一步是选择学生的尺寸。这几乎完全取决于你的目标部署硬件。",
    "tool_title": "交互式硬件推荐器",
    "label_vram": "可用显存 (GB)",
    "label_use": "主要用例",
    "opt_chat": "通用聊天",
    "opt_code": "代码助手",
    "opt_rag": "RAG (文档问答)",
    "btn_find": "查找最佳模型尺寸",
    "rec_title": "推荐",
    "tiers_title": "尺寸层级细分",
    "t1_title": "10亿 - 30亿 (1B - 3B) 参数",
    "t1_target": "<strong>目标：</strong> 手机，树莓派，基于浏览器的运行 (WebLLM)。",
    "t1_cap": "<strong>能力：</strong> 擅长语法纠正、简单分类和JSON格式化。在复杂推理或数学方面表现挣扎。非常适合特定的狭窄任务。",
    "t2_title": "70亿 - 90亿 (7B - 9B) 参数",
    "t2_target": "<strong>目标：</strong> 大多数消费级笔记本电脑 (M1/M2 MacBooks, NVIDIA GTX 1080+)。",
    "t2_cap": "<strong>能力：</strong> “甜蜜点”。具有不错的推理、编码和聊天能力。使用4-bit量化，适合6-8GB内存。这是通用蒸馏最常见的目标。",
    "t3_title": "130亿 - 300亿 (13B - 30B) 参数",
    "t3_target": "<strong>目标：</strong> 带有独立显卡的台式机 (建议24GB显存)。",
    "t3_cap": "<strong>能力：</strong> 接近GPT-3.5的性能。对于复杂的指令遵循、创意写作和细致的角色扮演是必要的。在标准笔记本电脑上很难运行。"
  },
  "data": {
    "title": "蒸馏数据",
    "lead": "垃圾进，垃圾出。你的学生模型的质量完全取决于教师生成的数据质量。",
    "syn_title": "合成数据生成",
    "syn_text": "大多数现代蒸馏依赖于“合成数据”——使用前沿模型（GPT-4, Claude 3.5）生成训练样本。这比人工标注更便宜、更快捷。",
    "pipe_title": "流程：“教科书”方法",
    "step_seed": "<strong>种子提示：</strong> 编写100个你想要的高质量任务示例（例如SQL查询）。",
    "step_exp": "<strong>扩展：</strong> 要求教师“生成10个类似这样的例子，但改变复杂度和领域。”",
    "step_filt": "<strong>过滤：</strong> 使用教师（或基于规则的系统）验证输出。SQL查询真的能执行吗？",
    "step_fmt": "<strong>格式化：</strong> 转换为学生的训练格式（ChatML, Alpaca等）。",
    "cot_title": "思维链 (CoT) 蒸馏",
    "cot_text": "如果你只训练学生记住答案，它会死记硬背。如果你训练它学习<em>推理过程</em>，它会学习逻辑。",
    "cot_bad": "<strong>坏数据：</strong> <code>问：2+2？ 答：4</code>",
    "cot_good": "<strong>好数据 (CoT)：</strong> <code>问：2+2？ 答：我们有两个单位。再加两个单位总共是四个单位。答案：4</code>",
    "cot_res": "研究表明，在训练数据中包含推理步骤可以显著提高学生泛化到新问题的能力。",
    "tip_title": "提示：多样性为王",
    "tip_text": "不要生成100,000个相同句法结构的例子。学生会过拟合。优先考虑长度、语气、复杂度和格式的多样性，而不是原始数量。"
  },
  "pipelines": {
    "title": "蒸馏流程",
    "lead": "将教师转化为学生的标准化工作流。点击下方步骤展开详情。",
    "step_1_title": "1. 定义范围与教师",
    "step_1_text": "选择你的任务（例如，“Python代码助手”）。选择以该能力闻名的教师（例如 GPT-4 或 DeepSeek-Coder-33B）。根据部署硬件决定学生尺寸（例如 7B）。",
    "step_2_title": "2. 数据策展",
    "step_2_text": "使用教师生成1万-5万个样本。包含思维链推理。清洗数据以去除“作为一名AI模型”的拒答。划分为训练/验证集。",
    "step_3_title": "3. 训练 (SFT)",
    "step_3_text": "在生成的数据集上微调学生。如果显存不足使用LoRA，如果有集群访问权限则全量微调。监控验证损失以防止过拟合。",
    "step_4_title": "4. 评估与迭代",
    "step_4_text": "运行基准测试（代码用HumanEval，知识用MMLU）。如果准确率低，回到第2步提高数据质量（更难的例子，更好的推理）。重复。"
  },
  "eval": {
    "title": "评估蒸馏模型",
    "lead": "你怎么知道你的学生是真的在学习，还是只是在死记硬背？评估是多维度的。",
    "quant_title": "1. 定量指标",
    "metric_ppl": "<strong>困惑度 (PPL)：</strong> 衡量模型对文本的惊讶程度。越低越好。适合检查语言连贯性。",
    "metric_bench": "<strong>基准测试：</strong> 标准测试如 MMLU (通用知识), HumanEval (编程), GSM8K (数学)。",
    "judge_title": "2. LLM作为裁判 (LLM-as-a-Judge)",
    "judge_text": "使用更强的模型（如GPT-4）给学生的回答打分。这比人工审查更具扩展性。",
    "calc_title": "指标计算器",
    "est_title": "性能影响估算器",
    "est_desc": "估算从教师迁移到学生时的性能下降。",
    "label_teacher": "教师得分 (MMLU)",
    "label_ratio": "压缩比率",
    "opt_10": "10% 尺寸 (10:1)",
    "opt_20": "20% 尺寸 (5:1)",
    "opt_50": "50% 尺寸 (2:1)",
    "btn_calc": "计算预期学生得分"
  },
  "cases": {
    "title": "实际权衡与案例研究",
    "lead": "蒸馏是妥协的艺术。探索现实世界的限制如何塑造工程决策。",
    "sim_title": "策略模拟器",
    "sim_desc": "配置部署场景以接收工程建议。",
    "label_device": "设备",
    "label_task": "任务",
    "label_latency": "延迟",
    "opt_mobile": "移动端 (2-4GB RAM)",
    "opt_laptop": "笔记本 (8-16GB RAM)",
    "opt_desktop": "台式机 (24GB VRAM)",
    "opt_simple": "简单格式/分类",
    "opt_coding": "编码/推理",
    "opt_creative": "创意/分析",
    "btn_run": "分析与推荐"
  },
  "glossary": {
    "title": "术语表",
    "search_placeholder": "搜索术语（例如 'Logits', 'KL散度'）...",
    "no_results": "未找到术语。",
    "term_activation": "神经元/层的输出。基于特征的蒸馏匹配这些。",
    "term_distillation": "通过软标签将知识从教师转移给学生。",
    "term_dark_knowledge": "隐藏在低概率logits中的信息（例如，狗与狼相似）。",
    "term_kl": "衡量教师/学生概率分布之间差异的损失函数。",
    "term_logits": "Softmax之前的原始预测分数。",
    "term_temp": "软化概率分布以揭示暗知识的超参数。"
  },
  "recommender": {
    "r1_title": "推荐：1B - 3B (量化)",
    "r1_text": "你的硬件非常受限。坚持使用像 TinyLlama-1.1B 或 Qwen-1.5B 这样的模型。量化到 4-bit (q4_k_m)。不要尝试大上下文窗口。",
    "r2_code_title": "推荐：DeepSeek-Coder-6.7B (Q4)",
    "r2_code_text": "对于编程，你至少需要60亿参数。拥有8GB显存，你可以舒适地运行4-bit量化的7B模型。",
    "r2_std_title": "推荐：Llama-3-8B (Q4/Q5)",
    "r2_std_text": "行业标准的8B模型完美适配8GB显存，使用4-bit或5-bit量化。",
    "r3_title": "推荐：13B - 14B (Q4)",
    "r3_text": "拥有12GB+显存，你可以升级到像 Mistral-Nemo-12B 或 Qwen-14B 这样的模型。这些模型提供比8B模型明显更好的推理能力。",
    "r4_title": "推荐：34B - 70B (Q4)",
    "r4_text": "拥有24GB+显存，你处于发烧友层级。你可以高精度运行 Yi-34B 或 command-r (35B)，甚至挑战量化的 Llama-3-70B (IQ2_XXS)。"
  },
  "simulator": {
    "m_code_title": "⚠️ 不推荐",
    "m_code_desc": "移动设备缺乏运行胜任的编程模型所需的内存。",
    "m_code_s1": "卸载到服务器",
    "m_code_s2": "仅使用微小的1B模型进行语法检查",
    "m_gen_title": "目标：1B-3B 量化",
    "m_gen_desc": "使用高度压缩的模型，如 TinyLlama 或 Qwen-1.5B。",
    "m_gen_s1": "使用 GGUF 格式 (q4_k_m)",
    "m_gen_s2": "预期 20 t/s",
    "l_title": "目标：7B-8B 模型",
    "l_desc": "甜蜜点。Llama-3-8B 或 Mistral-7B。",
    "l_s1": "使用 4-bit 量化",
    "l_s2": "适合通用聊天/推理",
    "d_title": "目标：13B-34B 模型",
    "d_desc": "你有高端硬件。利用它。",
    "d_s1": "Yi-34B 或 Command R",
    "d_s2": "全 FP16 或高位量化",
    "d_s3": "具备深度推理能力"
  },
  "eval_logic": {
    "result_label": "预期学生得分 (含蒸馏):",
    "base_label": "无蒸馏 (从头训练):"
  }
}