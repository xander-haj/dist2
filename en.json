{
  "brand": "Distill.Lab",
  "nav": {
    "curriculum": "Curriculum",
    "intro": "Introduction",
    "what_is": "What is Distillation?",
    "concepts": "Core Concepts",
    "vs": "Distillation vs Compression",
    "sizes": "Target Model Sizes",
    "data": "Data Engineering",
    "pipelines": "Distillation Pipelines",
    "eval": "Evaluation",
    "tradeoffs": "Tradeoffs & Cases",
    "glossary": "Glossary",
    "play_all": "Play All",
    "play_all_label": "Play Page",
    "voice_label": "Voice",
    "voice_auto": "Auto (Default)"
  },
  "home": {
    "title": "Engineering Efficient LLMs through Model Distillation",
    "lead": "A comprehensive, zero-to-hero guide on training small, efficient language models that mimic the performance of frontier giants.",
    "why_title": "Why Distillation Matters",
    "why_text": "Running a 70B parameter model locally requires ~$15,000 in GPU hardware (2x A6000 or 4x 3090). Distillation allows you to compress that intelligence into a 7B or 13B model that runs on a single consumer GPU or even a high-end laptop, retaining 90-95% of the teacher's specific capabilities.",
    "path_title": "Choose Your Learning Path",
    "beginner_title": "Beginner Path",
    "beginner_text": "Start from first principles. Understand the 'why' and 'how' before touching code.",
    "beginner_btn": "Start Here",
    "advanced_title": "Advanced Engineer",
    "advanced_text": "You know the theory. Jump straight to implementation details and benchmarks.",
    "advanced_btn": "Jump to Pipelines",
    "tip_title": "Tip for Developers",
    "tip_text": "If you are building a local assistant, do not start with a blank slate. Distilling a 70B model (like Llama-3-70B) into an 8B student often yields better results than fine-tuning the 8B model on human datasets alone."
  },
  "what_is": {
    "title": "What is Model Distillation?",
    "lead": "Model distillation is the process of compressing the complex knowledge of a massive 'Teacher' model into a compact 'Student' model, preserving accuracy while drastically reducing computational cost.",
    "intuition_title": "The Core Intuition",
    "intuition_text": "Imagine a university professor (the Teacher) who has spent 40 years studying physics. They know the formulas, but also the intuition, the edge cases, and the common misconceptions. A student (the Student model) doesn't need to re-live those 40 years. They can learn directly from the professor's lectures (the soft labels) and curated exams (the dataset).",
    "dl_terms": "In Deep Learning terms:",
    "teacher_def": "<strong>The Teacher:</strong> A massive model (e.g., GPT-4, Llama-3-70B) with high accuracy but huge latency and cost.",
    "student_def": "<strong>The Student:</strong> A smaller architecture (e.g., Llama-3-8B, TinyLlama-1B) that we want to train.",
    "knowledge_def": "<strong>The Knowledge:</strong> Not just the final answer ('Yes/No'), but the probability distribution over all possible answers (the 'Dark Knowledge').",
    "why_distill_title": "Why Distill?",
    "latency_title": "Latency",
    "latency_text": "A 70B model might generate 5 tokens/second on consumer hardware. A distilled 7B model can generate 50-100 tokens/second, enabling real-time chat and tool use.",
    "cost_title": "Cost",
    "cost_text": "Deploying a 70B model requires ~140GB of VRAM (multiple A100s). A 7B student fits on a single RTX 3060 or even a MacBook Air, reducing deployment costs by 10x-100x.",
    "expect_title": "Realistic Expectations",
    "expect_text": "Distillation is not magic. A 1B parameter student will never fully match the reasoning depth of a 70B teacher. However, for <em>specific tasks</em> (like summarizing news or converting JSON), the student can match or even exceed the teacher's reliability through focused training."
  },
  "concepts": {
    "title": "Core Concepts",
    "lead": "To successfully distill a model, you must understand the signals being transferred. It is not just about text; it is about probability distributions.",
    "logits_title": "1. Logits and Soft Labels",
    "logits_text_1": "Standard training uses 'Hard Labels' (Ground Truth). If the image is a cat, the target is 100% Cat, 0% Dog. However, this discards information. A wolf looks more like a dog than a cat.",
    "logits_text_2": "<strong>Logits</strong> are the raw scores output by the model before normalization. <strong>Soft Labels</strong> are the probabilities after applying Softmax. The Teacher might say: 'This is 90% Dog, 9% Wolf, 1% Cat'. This 9% tells the Student about visual similarity.",
    "temp_title": "Math: The Temperature Parameter ($T$)",
    "temp_desc": "To expose more 'Dark Knowledge' (the small probabilities), we divide the logits ($z_i$) by a temperature $T$ before Softmax:",
    "temp_li_1": "<strong>$T = 1$:</strong> Standard Softmax. Strong peaks, hides small classes.",
    "temp_li_2": "<strong>$T > 1$:</strong> Softens the distribution. The student sees that 'Wolf' is a plausible alternative to 'Dog'. This is crucial for distillation.",
    "loss_title": "2. Loss Functions",
    "loss_intro": "Distillation typically combines two loss functions:",
    "loss_kl": "<strong>Distillation Loss (KL Divergence):</strong> Measures how much the Student's soft probability distribution diverges from the Teacher's soft distribution.",
    "loss_ce": "<strong>Student Loss (Cross-Entropy):</strong> Measures accuracy against the hard ground truth labels (ensures the student doesn't just drift into abstract imitation).",
    "pitfall_title": "Pitfall: Vocabulary Mismatch",
    "pitfall_text": "Logit-based distillation requires the Teacher and Student to share the exact same Tokenizer. If they differ (e.g., Llama-3 vs Mistral), you cannot match logits directly. You must use <strong>Instruction Distillation</strong> (training on generated text) instead."
  },
  "vs": {
    "title": "Distillation vs. Other Compression Techniques",
    "lead": "Distillation is not the only way to make models smaller. It often works best when combined with Quantization and Pruning.",
    "table_title": "Comparison Table",
    "th_tech": "Technique",
    "th_mech": "Mechanism",
    "th_pros": "Pros",
    "th_cons": "Cons",
    "distill_desc": "Training a smaller architecture to mimic a larger one.",
    "distill_pros": "Changes architecture structure; faster inference; preserves logic well.",
    "distill_cons": "Expensive to train; requires good teacher data.",
    "quant_desc": "Reducing precision (e.g., FP16 to INT4).",
    "quant_pros": "Instant memory reduction; no training required (usually).",
    "quant_cons": "Loss of precision; doesn't increase speed as much as smaller architecture.",
    "prune_desc": "Removing neurons or layers.",
    "prune_pros": "Can create sparse models.",
    "prune_cons": "Often results in sparse matrices that are hard to accelerate on standard GPUs.",
    "lora_desc": "Freezing weights and training small adapters.",
    "lora_pros": "Very cheap training memory.",
    "lora_cons": "Does not reduce inference size (unless merged, and even then, base model size is fixed).",
    "pipeline_title": "The 'Distill-then-Quantize' Pipeline",
    "pipeline_intro": "The industry standard for local deployment is to combine these:",
    "step_1": "<strong>Distill:</strong> Train a 7B student from a 70B teacher in FP16.",
    "step_2": "<strong>Quantize:</strong> Convert the 7B student to 4-bit (GGUF/EXL2).",
    "pipeline_out": "This results in a model that is physically smaller (architecture) AND memory dense (precision), often running 4x-10x faster than the original teacher with surprisingly close performance."
  },
  "sizes": {
    "title": "Choosing Target Model Sizes",
    "lead": "The first step in distillation is choosing the student's size. This is dictated almost entirely by your target deployment hardware.",
    "tool_title": "Interactive Hardware Recommender",
    "label_vram": "Available VRAM (GB)",
    "label_use": "Primary Use Case",
    "opt_chat": "General Chat",
    "opt_code": "Coding Assistant",
    "opt_rag": "RAG (Document QA)",
    "btn_find": "Find Best Model Size",
    "rec_title": "Recommendation",
    "tiers_title": "Size Tiers Breakdown",
    "t1_title": "1B - 3B Parameters",
    "t1_target": "<strong>Target:</strong> Phones, Raspberry Pi, Browser-based (WebLLM).",
    "t1_cap": "<strong>Capabilities:</strong> Good for grammar correction, simple classification, and JSON formatting. Struggles with complex reasoning or math. Perfect for specific, narrow tasks.",
    "t2_title": "7B - 9B Parameters",
    "t2_target": "<strong>Target:</strong> Most consumer laptops (M1/M2 MacBooks, NVIDIA GTX 1080+).",
    "t2_cap": "<strong>Capabilities:</strong> The 'Sweet Spot.' Capable of decent reasoning, coding, and chat. With 4-bit quantization, fits in 6-8GB RAM. This is the most common target for general purpose distillation.",
    "t3_title": "13B - 30B Parameters",
    "t3_target": "<strong>Target:</strong> Desktops with dedicated GPUs (24GB VRAM recommended).",
    "t3_cap": "<strong>Capabilities:</strong> Near-GPT-3.5 performance. Necessary for complex instruction following, creative writing, and nuanced roleplay. Hard to run on standard laptops."
  },
  "data": {
    "title": "Data for Distillation",
    "lead": "Garbage In, Garbage Out. The quality of your Student model depends entirely on the quality of the data generated by your Teacher.",
    "syn_title": "Synthetic Data Generation",
    "syn_text": "Most modern distillation relies on 'Synthetic Data' — using a frontier model (GPT-4, Claude 3.5) to generate training examples. This is cheaper and faster than human annotation.",
    "pipe_title": "Pipeline: The 'Textbook' Approach",
    "step_seed": "<strong>Seed Prompts:</strong> Write 100 high-quality examples of the task you want (e.g., SQL queries).",
    "step_exp": "<strong>Expansion:</strong> Ask the Teacher to 'Generate 10 more examples like these, but vary the complexity and domain.'",
    "step_filt": "<strong>Filtering:</strong> Use the Teacher (or a rule-based system) to verify the output. Does the SQL query actually execute?",
    "step_fmt": "<strong>Formatting:</strong> Convert into the Student's training format (ChatML, Alpaca, etc.).",
    "cot_title": "Chain-of-Thought (CoT) Distillation",
    "cot_text": "If you just train a Student on the answer, it memorizes the result. If you train it on the <em>reasoning</em>, it learns the logic.",
    "cot_bad": "<strong>Bad Data:</strong> <code>Q: 2+2? A: 4</code>",
    "cot_good": "<strong>Good Data (CoT):</strong> <code>Q: 2+2? A: We have two units. Adding two more units results in four units total. Answer: 4</code>",
    "cot_res": "Research shows that including the reasoning steps in the training data significantly improves the Student's ability to generalize to new problems.",
    "tip_title": "Tip: Diversity is King",
    "tip_text": "Do not generate 100,000 examples of the same sentence structure. The Student will overfit. Prioritize variety in length, tone, complexity, and formatting over raw volume."
  },
  "pipelines": {
    "title": "Distillation Pipelines",
    "lead": "A standardized workflow for taking a Teacher and producing a Student. Click the steps below to expand details.",
    "step_1_title": "1. Define Scope & Teacher",
    "step_1_text": "Choose your task (e.g., 'Python Coding Assistant'). Select a Teacher known for this capability (e.g., GPT-4 or DeepSeek-Coder-33B). Decide on the Student size (e.g., 7B) based on deployment hardware.",
    "step_2_title": "2. Data Curation",
    "step_2_text": "Generate 10k-50k samples using the Teacher. Include Chain-of-Thought reasoning. Clean the data to remove 'As an AI model' refusals. Split into Train/Validation sets.",
    "step_3_title": "3. Training (SFT)",
    "step_3_text": "Fine-Tune the Student on the generated dataset. Use LoRA if GPU poor, or Full Fine-Tuning if you have cluster access. Monitor Validation Loss to prevent overfitting.",
    "step_4_title": "4. Evaluation & Iteration",
    "step_4_text": "Run benchmarks (HumanEval for code, MMLU for knowledge). If accuracy is low, go back to Step 2 and improve data quality (harder examples, better reasoning). Repeat."
  },
  "eval": {
    "title": "Evaluating Distilled Models",
    "lead": "How do you know if your Student is actually learning, or just memorizing? Evaluation is multi-dimensional.",
    "quant_title": "1. Quantitative Metrics",
    "metric_ppl": "<strong>Perplexity (PPL):</strong> Measures how surprised the model is by text. Lower is better. Good for checking linguistic coherence.",
    "metric_bench": "<strong>Benchmarks:</strong> Standard tests like MMLU (General Knowledge), HumanEval (Coding), GSM8K (Math).",
    "judge_title": "2. LLM-as-a-Judge",
    "judge_text": "Use a stronger model (like GPT-4) to grade the Student's answers. This scales better than human review.",
    "calc_title": "Metric Calculator",
    "est_title": "Performance Impact Estimator",
    "est_desc": "Estimate the drop in performance when moving from Teacher to Student.",
    "label_teacher": "Teacher Score (MMLU)",
    "label_ratio": "Compression Ratio",
    "opt_10": "10% Size (10:1)",
    "opt_20": "20% Size (5:1)",
    "opt_50": "50% Size (2:1)",
    "btn_calc": "Calculate Expected Student Score"
  },
  "cases": {
    "title": "Practical Tradeoffs & Case Studies",
    "lead": "Distillation is the art of compromise. Explore how real-world constraints shape engineering decisions.",
    "sim_title": "Strategy Simulator",
    "sim_desc": "Configure a deployment scenario to receive an engineering recommendation.",
    "label_device": "Device",
    "label_task": "Task",
    "label_latency": "Latency",
    "opt_mobile": "Mobile (2-4GB RAM)",
    "opt_laptop": "Laptop (8-16GB RAM)",
    "opt_desktop": "Desktop (24GB VRAM)",
    "opt_simple": "Simple Format/Classify",
    "opt_coding": "Coding/Reasoning",
    "opt_creative": "Creative/Analysis",
    "btn_run": "Analyze & Recommend"
  },
  "glossary": {
    "title": "Glossary of Terms",
    "search_placeholder": "Search terms (e.g., 'Logits', 'KL Divergence')...",
    "no_results": "No terms found.",
    "term_activation": "The output of a neuron/layer. Feature-based distillation matches these.",
    "term_distillation": "Transferring knowledge from Teacher to Student via soft labels.",
    "term_dark_knowledge": "Information hidden in low-probability logits (e.g., Dog is similar to Wolf).",
    "term_kl": "Loss function measuring difference between Teacher/Student probability distributions.",
    "term_logits": "Raw prediction scores before Softmax.",
    "term_temp": "Hyperparameter to soften probability distributions, revealing Dark Knowledge."
  },
  "recommender": {
    "r1_title": "Recommendation: 1B - 3B (Quantized)",
    "r1_text": "Your hardware is very constrained. Stick to models like TinyLlama-1.1B or Qwen-1.5B. Quantize to 4-bit (q4_k_m). Do not attempt large context windows.",
    "r2_code_title": "Recommendation: DeepSeek-Coder-6.7B (Q4)",
    "r2_code_text": "For coding, you need at least 6B parameters. With 8GB VRAM, you can run a 4-bit quantized 7B model comfortably.",
    "r2_std_title": "Recommendation: Llama-3-8B (Q4/Q5)",
    "r2_std_text": "The industry standard 8B model fits perfectly in 8GB VRAM with 4-bit or 5-bit quantization.",
    "r3_title": "Recommendation: 13B - 14B (Q4)",
    "r3_text": "With 12GB+ VRAM, you can step up to models like Mistral-Nemo-12B or Qwen-14B. These offer significantly better reasoning than 8B models.",
    "r4_title": "Recommendation: 34B - 70B (Q4)",
    "r4_text": "With 24GB+ VRAM, you are in the enthusiast tier. You can run Yi-34B or command-r (35B) at high precision, or even a quantized Llama-3-70B (IQ2_XXS) if you push limits."
  },
  "simulator": {
    "m_code_title": "⚠️ Not Recommended",
    "m_code_desc": "Mobile devices lack the RAM for competent coding models.",
    "m_code_s1": "Offload to server",
    "m_code_s2": "Use a tiny 1B model for syntax only",
    "m_gen_title": "Target: 1B-3B Quantized",
    "m_gen_desc": "Use a highly compressed model like TinyLlama or Qwen-1.5B.",
    "m_gen_s1": "Use GGUF format (q4_k_m)",
    "m_gen_s2": "Expect 20 t/s",
    "l_title": "Target: 7B-8B Model",
    "l_desc": "The sweet spot. Llama-3-8B or Mistral-7B.",
    "l_s1": "Use 4-bit quantization",
    "l_s2": "Good for general chat/reasoning",
    "d_title": "Target: 13B-34B Model",
    "d_desc": "You have high-end hardware. Use it.",
    "d_s1": "Yi-34B or Command R",
    "d_s2": "Full FP16 or high-bit quant",
    "d_s3": "Deep reasoning capable"
  },
  "eval_logic": {
    "result_label": "Expected Student Score (with Distillation):",
    "base_label": "Without distillation (Training from scratch):"
  }
}